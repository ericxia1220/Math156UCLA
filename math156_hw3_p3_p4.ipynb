{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, batch_size, learning_rate=0.0003, num_epochs=1000): \n",
    "    np.random.seed(RANDOM_STATE)\n",
    "\n",
    "    # add bias and initialize weights with standard guassian\n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "    w = np.random.randn(X.shape[1])\n",
    "\n",
    "    # train the model\n",
    "    for epoch in range(num_epochs): \n",
    "        # shuffle the data\n",
    "        indices = np.arange(len(X))\n",
    "        np.random.shuffle(indices)\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "\n",
    "        # mini-batch sgd \n",
    "        for i in range(0, len(X), batch_size): \n",
    "            X_batch = X[i:i+batch_size]\n",
    "            y_batch = y[i:i+batch_size]\n",
    "\n",
    "            # forward pass  \n",
    "            logits = np.dot(X_batch, w)\n",
    "            y_pred = 1 / (1 + np.exp(-logits))\n",
    "\n",
    "            # compute gradient \n",
    "            grad = np.dot(X_batch.T, y_pred - y_batch)\n",
    "            w -= learning_rate * grad\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  (569, 30)\n",
      "Target shape:  (569,)\n"
     ]
    }
   ],
   "source": [
    "# load wisconsin breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data \n",
    "y = data.target\n",
    "print(\"Data shape: \", X.shape)\n",
    "print(\"Target shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (398, 30) (398,)\n",
      "Val:  (85, 30) (85,)\n",
      "Test:  (86, 30) (86,)\n"
     ]
    }
   ],
   "source": [
    "# split ratios\n",
    "train_ratio = 0.70\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Use train_test_split to split the data into training, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 - train_ratio, random_state=RANDOM_STATE)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=test_ratio/(test_ratio + val_ratio), random_state=RANDOM_STATE) \n",
    "\n",
    "# standardize data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"Train: \", X_train.shape, y_train.shape)\n",
    "print(\"Val: \", X_val.shape, y_val.shape)\n",
    "print(\"Test: \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 class size:  186\n",
      "Label 1 class size:  297\n"
     ]
    }
   ],
   "source": [
    "class_sizes = np.unique(np.concatenate((y_train, y_val)), return_counts=True)\n",
    "\n",
    "print(\"Label 0 class size: \", class_sizes[1][0])\n",
    "print(\"Label 1 class size: \", class_sizes[1][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0003, Batch size: 16, Validation loss: 0.055431828761968255\n",
      "Learning rate: 0.0003, Batch size: 32, Validation loss: 0.055431676564869456\n",
      "Learning rate: 0.0003, Batch size: 64, Validation loss: 0.055431225393166576\n",
      "Learning rate: 0.0003, Batch size: 128, Validation loss: 0.055430322510303616\n",
      "Learning rate: 0.0003, Batch size: 256, Validation loss: 0.05542926196690215\n",
      "Learning rate: 0.001, Batch size: 16, Validation loss: 0.060011764024911526\n",
      "Learning rate: 0.001, Batch size: 32, Validation loss: 0.06001127372453017\n",
      "Learning rate: 0.001, Batch size: 64, Validation loss: 0.060009568268255646\n",
      "Learning rate: 0.001, Batch size: 128, Validation loss: 0.06000853531157793\n",
      "Learning rate: 0.001, Batch size: 256, Validation loss: 0.06000752171909894\n",
      "Learning rate: 0.003, Batch size: 16, Validation loss: 0.06932122371537591\n",
      "Learning rate: 0.003, Batch size: 32, Validation loss: 0.0693159390014663\n",
      "Learning rate: 0.003, Batch size: 64, Validation loss: 0.0693034097357649\n",
      "Learning rate: 0.003, Batch size: 128, Validation loss: 0.06930052744500613\n",
      "Learning rate: 0.003, Batch size: 256, Validation loss: 0.06928615342253482\n",
      "Learning rate: 0.01, Batch size: 16, Validation loss: 0.09003051325448942\n",
      "Learning rate: 0.01, Batch size: 32, Validation loss: 0.09001981170141754\n",
      "Learning rate: 0.01, Batch size: 64, Validation loss: 0.08997009098407896\n",
      "Learning rate: 0.01, Batch size: 128, Validation loss: 0.09001025169557818\n",
      "Learning rate: 0.01, Batch size: 256, Validation loss: 0.08998136890591275\n",
      "Best learning rate: 0.0003, Best batch size: 256, Best validation loss: 0.05542926196690215\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy_loss(y_true, y_pred): \n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def predict(X, w): \n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "    return 1 / (1 + np.exp(-np.dot(X, w)))\n",
    "\n",
    "# grid search for learning rate and batch size\n",
    "learning_rates = [0.0003, 0.001, 0.003, 0.01]\n",
    "batch_sizes = [16, 32, 64, 128, 256] \n",
    "num_epochs = 1000\n",
    "lowest_val_loss = float('inf')\n",
    "best_w = None\n",
    "for lr in learning_rates: \n",
    "    for batch_size in batch_sizes: \n",
    "        w_current = logistic_regression(X_train, y_train, batch_size, lr, num_epochs)\n",
    "        # get loss on validation set\n",
    "        y_val_pred = predict(X_val, w_current)\n",
    "        val_loss = cross_entropy_loss(y_val, y_val_pred)\n",
    "        # update best model\n",
    "        if val_loss < lowest_val_loss: \n",
    "            lowest_val_loss = val_loss\n",
    "            best_lr = lr\n",
    "            best_batch_size = batch_size\n",
    "            best_w = w_current\n",
    "\n",
    "        print(f\"Learning rate: {lr}, Batch size: {batch_size}, Validation loss: {val_loss}\")\n",
    "\n",
    "print(f\"Best learning rate: {best_lr}, Best batch size: {best_batch_size}, Best validation loss: {lowest_val_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9883720930232558, Precision: 0.9836065573770492, Recall: 1.0, F1: 0.9917355371900827\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "y_pred = predict(X_test, best_w)\n",
    "accuracy = accuracy_score(y_test, y_pred > 0.5)\n",
    "precision = precision_score(y_test, y_pred > 0.5)\n",
    "recall = recall_score(y_test, y_pred > 0.5)\n",
    "f1 = f1_score(y_test, y_pred > 0.5)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarization: Overall the model performs well on the test set, with a high performance across the four eval metrics. The overall accuracy is 98.84%. With a precision of 98.36%, the model results in maybe a few false positives. The perfect recall score indicates that there is no false negatives. Through grid search on learning rate and batch size, we found that the model performed the best when lr=0.0003 and batch_size = 256. But in general, it seems for this problem, the model performs well for smaller learning rates with larger batch sizes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math156ucla-on3gjILV-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
